{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FuKdUXdUb8t"
      },
      "source": [
        "# Fine-tuning a model with the Trainer API or Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5oECNwaUb8v"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XS9BkJvOUb8v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (2.14.7)\n",
            "Requirement already satisfied: evaluate in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (0.4.1)\n",
            "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (4.35.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (1.26.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (14.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (0.5)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: responses<0.19 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from transformers[sentencepiece]) (4.25.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: colorama in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\choyd\\onedrive\\documents\\projects\\hf_trainer_api\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EKCM55NRUb8v"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Documentation for TrainingArguments can be [found here](https://huggingface.co/docs/transformers/v4.35.2/en/main_classes/trainer#transformers.TrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dj0UBfKqUb8w"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"test-trainer\", num_train_epochs=5, lr_scheduler_type=\"cosine_with_restarts\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SbwB-K3pUb8w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i6vnfdfAUb8w"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i7pWw1ooUb8w"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2295 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 22%|██▏       | 500/2295 [00:22<01:20, 22.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4996, 'learning_rate': 4.4369285612260874e-05, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 1000/2295 [00:45<00:57, 22.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2784, 'learning_rate': 3.001353801034688e-05, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 1500/2295 [01:09<00:36, 22.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1211, 'learning_rate': 1.339940635976592e-05, 'epoch': 3.27}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 2000/2295 [01:32<00:13, 22.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0399, 'learning_rate': 2.0108450704954348e-06, 'epoch': 4.36}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2295/2295 [01:46<00:00, 21.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 106.5187, 'train_samples_per_second': 172.176, 'train_steps_per_second': 21.546, 'train_loss': 0.20613847485295048, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2295, training_loss=0.20613847485295048, metrics={'train_runtime': 106.5187, 'train_samples_per_second': 172.176, 'train_steps_per_second': 21.546, 'train_loss': 0.20613847485295048, 'epoch': 5.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3zBjG0U6Ub8x",
        "outputId": "44e12c70-23e7-49d5-b17c-b77c5017b742"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [00:00<00:00, 132.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(408, 2) (408,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DvYM0p67Ub8x",
        "outputId": "f3b7be69-e276-4338-8114-fb6a71d3ee50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8553921568627451, 'f1': 0.8998302207130731}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xEexacyCUb8x"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hkX0zCtcUb8x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MyXD5fBMUb8y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 33%|███▎      | 460/1377 [00:23<04:33,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5414108037948608, 'eval_accuracy': 0.7892156862745098, 'eval_f1': 0.8599348534201955, 'eval_runtime': 2.5174, 'eval_samples_per_second': 162.074, 'eval_steps_per_second': 20.259, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▋      | 500/1377 [00:25<00:42, 20.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5851, 'learning_rate': 3.184458968772695e-05, 'epoch': 1.09}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \n",
            " 67%|██████▋   | 918/1377 [00:47<00:20, 22.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5383989214897156, 'eval_accuracy': 0.8382352941176471, 'eval_f1': 0.8918032786885245, 'eval_runtime': 2.4851, 'eval_samples_per_second': 164.182, 'eval_steps_per_second': 20.523, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 1000/1377 [00:51<00:16, 22.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3924, 'learning_rate': 1.3689179375453886e-05, 'epoch': 2.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                   \n",
            "100%|██████████| 1377/1377 [01:11<00:00, 19.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5481570959091187, 'eval_accuracy': 0.8651960784313726, 'eval_f1': 0.9056603773584906, 'eval_runtime': 2.492, 'eval_samples_per_second': 163.722, 'eval_steps_per_second': 20.465, 'epoch': 3.0}\n",
            "{'train_runtime': 71.5005, 'train_samples_per_second': 153.901, 'train_steps_per_second': 19.259, 'train_loss': 0.42891574546575717, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1377, training_loss=0.42891574546575717, metrics={'train_runtime': 71.5005, 'train_samples_per_second': 153.901, 'train_steps_per_second': 19.259, 'train_loss': 0.42891574546575717, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
